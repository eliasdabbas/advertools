

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>advertools.robotstxt &mdash;  Python</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> advertools
          

          
          </a>

          
            
            
              <div class="version">
                0.10.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../readme.html">About advertools</a></li>
</ul>
<p class="caption"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption"><span class="caption-text">SEO</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.robotstxt.html">robots.txt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.spider.html">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.knowledge_graph.html">Google's Knowledge Graph</a></li>
</ul>
<p class="caption"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.urlytics.html">URL Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../include_changelog.html">Index &amp; Change Log</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">advertools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>advertools.robotstxt</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for advertools.robotstxt</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">.. _robotstxt:</span>

<span class="sd">ðŸ¤– robots.txt Tester for Large Scale Testing</span>
<span class="sd">============================================</span>

<span class="sd">Even though they are tiny in size, robots.txt files contain potent information</span>
<span class="sd">that can block major sections of your site, which is what they are supposed to</span>
<span class="sd">do. Only sometimes you might make the mistake of blocking the wrong section.</span>

<span class="sd">So it is very important to check if certain pages (or groups of pages) are</span>
<span class="sd">blocked for a certain user-agent by a certain robots.txt file. Ideally, you</span>
<span class="sd">would want to run the same check for all possible user-agents. Even more</span>
<span class="sd">ideally, you want to be able to run the check for a large number of pages with</span>
<span class="sd">every possible combination with user-agents!</span>

<span class="sd">To get the robots.txt file into an easily readable format, you can use the</span>
<span class="sd">:func:`robotstxt_to_df` function to get it in a DataFrame.</span>

<span class="sd">&gt;&gt;&gt; robotstxt_to_df(&#39;https://www.google.com/robots.txt&#39;)</span>
<span class="sd">      directive                             content                      robotstxt_url                  file_downloaded</span>
<span class="sd">0    User-agent                                   *  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">1      Disallow                             /search  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">2         Allow                       /search/about  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">3         Allow                      /search/static  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">4         Allow              /search/howsearchworks  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">..          ...                                 ...                                ...                              ...</span>
<span class="sd">277  User-agent                          Twitterbot  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">278       Allow                             /imgres  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">279  User-agent                 facebookexternalhit  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">280       Allow                             /imgres  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">281     Sitemap  https://www.google.com/sitemap.xml  https://www.google.com/robots.txt 2020-06-01 14:05:16.068031+00:00</span>
<span class="sd">[282 rows x 4 columns]</span>


<span class="sd">The returned DataFrame contains columns for directives, their content, the URL</span>
<span class="sd">of the robots.txt file, as well as the date it was downloaded.</span>
<span class="sd">Under the `directive` column you can see the main commands; Allow, Disallow,</span>
<span class="sd">Sitemap, Crawl-delay, User-agent, and so on. The `content` column contains the</span>
<span class="sd">details of each of those directives (the pattern to disallow, the sitemap URL,</span>
<span class="sd">etc.)</span>

<span class="sd">As for testing, the :func:`robotstxt_test` function runs a test for a given</span>
<span class="sd">robots.txt file, checking which of the provided user-agents can fetch which of</span>
<span class="sd">the provided URLs, paths, or patterns.</span>

<span class="sd">&gt;&gt;&gt; robotstxt_test(&#39;https://www.example.com/robots.txt&#39;,</span>
<span class="sd">...                useragents=[&#39;Googlebot&#39;, &#39;baiduspider&#39;, &#39;Bingbot&#39;]</span>
<span class="sd">...                urls=[&#39;/&#39;, &#39;/hello&#39;, &#39;/some-page.html&#39;]])</span>

<span class="sd">As a result, you get a DataFrame with a row for each combination of</span>
<span class="sd">(user-agent, URL) indicating whether or not that particular user-agent can</span>
<span class="sd">fetch the given URL.</span>

<span class="sd">Some reasons why you might want to do that:</span>

<span class="sd">* SEO Audits: Especially for large websites with many URL patterns, and many</span>
<span class="sd">  rules for different user-agents.</span>
<span class="sd">* Developer or site owner about to make large changes</span>
<span class="sd">* Interest in strategies of certain companies</span>

<span class="sd">User-agents</span>
<span class="sd">-----------</span>

<span class="sd">In reality there are only two groups of user-agents that you need to worry</span>
<span class="sd">about:</span>

<span class="sd">* User-agents listed in the robots.txt file: For each one of those you need to</span>
<span class="sd">  check whether or not they are blocked from fetching a certain URL</span>
<span class="sd">  (or pattern).</span>
<span class="sd">* ``*`` all other user-agents: The ``*`` includes all other user-agents, so</span>
<span class="sd">  checking the rules that apply to it should take care of the rest.</span>

<span class="sd">robots.txt Testing Approach</span>
<span class="sd">---------------------------</span>

<span class="sd">1. Get the robots.txt file that you are interested in</span>
<span class="sd">2. Extract the user-agents from it</span>
<span class="sd">3. Specify the URLs you are interested in testing</span>
<span class="sd">4. Run the :func:`robotstxt_test` function</span>

<span class="sd">&gt;&gt;&gt; fb_robots = robotstxt_to_df(&#39;https://www.facebook.com/robots.txt&#39;)</span>
<span class="sd">&gt;&gt;&gt; fb_robots</span>
<span class="sd">      directive                                            content                        robotstxt_url                  file_downloaded</span>
<span class="sd">0       comment  Notice: Collection of data on Facebook through...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">1       comment  prohibited unless you have express written per...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">2       comment  and may only be conducted for the limited purp...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">3       comment                                        permission.  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">4       comment  See: http://www.facebook.com/apps/site_scrapin...  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">..          ...                                                ...                                  ...                              ...</span>
<span class="sd">461       Allow                         /ajax/bootloader-endpoint/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">462       Allow  /ajax/pagelet/generic.php/PagePostsSectionPagelet  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">463       Allow                                      /safetycheck/  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">464  User-agent                                                  *  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">465    Disallow                                                  /  https://www.facebook.com/robots.txt 2020-05-31 20:12:47.576281+00:00</span>
<span class="sd">[466 rows x 4 columns]</span>

<span class="sd">Now that we have downloaded the file, we can easily extract the list of</span>
<span class="sd">user-agents that it contains.</span>

<span class="sd">&gt;&gt;&gt; fb_useragents = (fb_robots</span>
<span class="sd">...                  [fb_robots[&#39;directive&#39;]==&#39;User-agent&#39;]</span>
<span class="sd">...                  [&#39;content&#39;].drop_duplicates()</span>
<span class="sd">...                  .tolist())</span>
<span class="sd">&gt;&gt;&gt; fb_useragents</span>
<span class="sd">[&#39;Applebot&#39;,</span>
<span class="sd"> &#39;baiduspider&#39;,</span>
<span class="sd"> &#39;Bingbot&#39;,</span>
<span class="sd"> &#39;Discordbot&#39;,</span>
<span class="sd"> &#39;facebookexternalhit&#39;,</span>
<span class="sd"> &#39;Googlebot&#39;,</span>
<span class="sd"> &#39;Googlebot-Image&#39;,</span>
<span class="sd"> &#39;ia_archiver&#39;,</span>
<span class="sd"> &#39;LinkedInBot&#39;,</span>
<span class="sd"> &#39;msnbot&#39;,</span>
<span class="sd"> &#39;Naverbot&#39;,</span>
<span class="sd"> &#39;Pinterestbot&#39;,</span>
<span class="sd"> &#39;seznambot&#39;,</span>
<span class="sd"> &#39;Slurp&#39;,</span>
<span class="sd"> &#39;teoma&#39;,</span>
<span class="sd"> &#39;TelegramBot&#39;,</span>
<span class="sd"> &#39;Twitterbot&#39;,</span>
<span class="sd"> &#39;Yandex&#39;,</span>
<span class="sd"> &#39;Yeti&#39;,</span>
<span class="sd"> &#39;*&#39;]</span>


<span class="sd">Quite a long list!</span>

<span class="sd">As a small and quick test, I&#39;m interested in checking the home page, a random</span>
<span class="sd">profile page (/bbc), groups and hashtags pages.</span>

<span class="sd">&gt;&gt;&gt; urls_to_test = [&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;]</span>
<span class="sd">&gt;&gt;&gt; fb_test = robotstxt_test(&#39;https://www.facebook.com/robots.txt&#39;,</span>
<span class="sd">...                          fb_useragents, urls_to_test)</span>
<span class="sd">&gt;&gt;&gt; fb_test</span>
<span class="sd">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="sd">0   https://www.facebook.com/robots.txt          *       /bbc      False</span>
<span class="sd">1   https://www.facebook.com/robots.txt          *    /groups      False</span>
<span class="sd">2   https://www.facebook.com/robots.txt          *          /      False</span>
<span class="sd">3   https://www.facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="sd">4   https://www.facebook.com/robots.txt   Applebot          /       True</span>
<span class="sd">..                                  ...        ...        ...        ...</span>
<span class="sd">75  https://www.facebook.com/robots.txt  seznambot    /groups       True</span>
<span class="sd">76  https://www.facebook.com/robots.txt      teoma          /       True</span>
<span class="sd">77  https://www.facebook.com/robots.txt      teoma  /hashtag/      False</span>
<span class="sd">78  https://www.facebook.com/robots.txt      teoma       /bbc       True</span>
<span class="sd">79  https://www.facebook.com/robots.txt      teoma    /groups       True</span>
<span class="sd">[80 rows x 4 columns]</span>


<span class="sd">For twenty user-agents and four URLs each, we received a total of eighty test</span>
<span class="sd">results. You can immediately see that all user-agents not listed (denoted by</span>
<span class="sd">`*` are not allowed to fetch any of the provided URLs).</span>

<span class="sd">Let&#39;s see who is and who is not allowed to fetch the home page.</span>

<span class="sd">&gt;&gt;&gt; fb_test.query(&#39;url_path== &quot;/&quot;&#39;)</span>
<span class="sd">                          robotstxt_url           user_agent  url_path  can_fetch</span>
<span class="sd">2   https://www.facebook.com/robots.txt                    *         /      False</span>
<span class="sd">4   https://www.facebook.com/robots.txt             Applebot         /       True</span>
<span class="sd">9   https://www.facebook.com/robots.txt              Bingbot         /       True</span>
<span class="sd">14  https://www.facebook.com/robots.txt           Discordbot         /      False</span>
<span class="sd">18  https://www.facebook.com/robots.txt            Googlebot         /       True</span>
<span class="sd">21  https://www.facebook.com/robots.txt      Googlebot-Image         /       True</span>
<span class="sd">26  https://www.facebook.com/robots.txt          LinkedInBot         /      False</span>
<span class="sd">30  https://www.facebook.com/robots.txt             Naverbot         /       True</span>
<span class="sd">35  https://www.facebook.com/robots.txt         Pinterestbot         /      False</span>
<span class="sd">39  https://www.facebook.com/robots.txt                Slurp         /       True</span>
<span class="sd">43  https://www.facebook.com/robots.txt          TelegramBot         /      False</span>
<span class="sd">47  https://www.facebook.com/robots.txt           Twitterbot         /       True</span>
<span class="sd">48  https://www.facebook.com/robots.txt               Yandex         /       True</span>
<span class="sd">55  https://www.facebook.com/robots.txt                 Yeti         /       True</span>
<span class="sd">57  https://www.facebook.com/robots.txt          baiduspider         /       True</span>
<span class="sd">60  https://www.facebook.com/robots.txt  facebookexternalhit         /      False</span>
<span class="sd">64  https://www.facebook.com/robots.txt          ia_archiver         /      False</span>
<span class="sd">68  https://www.facebook.com/robots.txt               msnbot         /       True</span>
<span class="sd">74  https://www.facebook.com/robots.txt            seznambot         /       True</span>
<span class="sd">76  https://www.facebook.com/robots.txt                teoma         /       True</span>

<span class="sd">I&#39;ll leave it to you to figure out why LinkedIn and Pinterest are not allowed</span>
<span class="sd">to crawl the home page but Google and Apple are, because I have no clue!</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;robotstxt_to_df&#39;</span><span class="p">,</span> <span class="s1">&#39;robotstxt_test&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="k">import</span> <span class="n">Request</span><span class="p">,</span> <span class="n">urlopen</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">product</span>

<span class="kn">from</span> <span class="nn">protego</span> <span class="k">import</span> <span class="n">Protego</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">advertools</span> <span class="k">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">version</span>

<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;advertools-&#39;</span> <span class="o">+</span> <span class="n">version</span><span class="p">}</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<div class="viewcode-block" id="robotstxt_to_df"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_to_df">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_to_df</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Download the contents of ``robotstxt_url`` into a DataFrame</span>

<span class="sd">    :param url robotstxt_url: The URL of the robots.txt file</span>
<span class="sd">    :returns DataFrame robotstxt_df: A DataFrame containing directives, their</span>
<span class="sd">                                     content, the URL and time of download</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="s1">&#39;Getting: &#39;</span> <span class="o">+</span> <span class="n">robotstxt_url</span><span class="p">)</span>
    <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">))</span>
    <span class="n">robots_text</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_text</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">):</span>
                <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s1">&#39;comment&#39;</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()])</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;directive&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">])</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">robotstxt_url</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;download_date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="o">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="s1">&#39;UTC&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>


<div class="viewcode-block" id="robotstxt_test"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_test">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_test</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">,</span> <span class="n">urls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a :attr:`robotstxt_url` check which of the :attr:`user_agents` is</span>
<span class="sd">    allowed to fetch which of the :attr:`urls`.</span>

<span class="sd">    All the combinations of :attr:`user_agents` and :attr:`urls` will be</span>
<span class="sd">    checked and the results returned in one DataFrame.</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_test(&#39;https://facebook.com/robots.txt&#39;,</span>
<span class="sd">    ...                user_agents=[&#39;*&#39;, &#39;Googlebot&#39;, &#39;Applebot&#39;],</span>
<span class="sd">    ...                urls=[&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;])</span>
<span class="sd">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="sd">    0   https://facebook.com/robots.txt          *          /      False</span>
<span class="sd">    1   https://facebook.com/robots.txt          *       /bbc      False</span>
<span class="sd">    2   https://facebook.com/robots.txt          *    /groups      False</span>
<span class="sd">    3   https://facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="sd">    4   https://facebook.com/robots.txt   Applebot          /       True</span>
<span class="sd">    5   https://facebook.com/robots.txt   Applebot       /bbc       True</span>
<span class="sd">    6   https://facebook.com/robots.txt   Applebot    /groups       True</span>
<span class="sd">    7   https://facebook.com/robots.txt   Applebot  /hashtag/      False</span>
<span class="sd">    8   https://facebook.com/robots.txt  Googlebot          /       True</span>
<span class="sd">    9   https://facebook.com/robots.txt  Googlebot       /bbc       True</span>
<span class="sd">    10  https://facebook.com/robots.txt  Googlebot    /groups       True</span>
<span class="sd">    11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False</span>

<span class="sd">    :param url robotstxt_url: The URL of robotx.txt file</span>
<span class="sd">    :param str,list user_agents: One or more user agents</span>
<span class="sd">    :param str,list urls: One or more paths (relative) or URLs (absolute) to</span>
<span class="sd">                           check</span>
<span class="sd">    :return DataFrame robotstxt_test_df:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">robotstxt_url</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;/robots.txt&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please make sure you enter a valid robots.txt URL&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_agents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">user_agents</span> <span class="o">=</span> <span class="p">[</span><span class="n">user_agents</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="n">urls</span><span class="p">]</span>
    <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">))</span>
    <span class="n">robots_bytes</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">robots_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_bytes</span><span class="p">)</span>
    <span class="n">rp</span> <span class="o">=</span> <span class="n">Protego</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">robots_text</span><span class="p">)</span>

    <span class="n">test_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;user_agent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;url_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;can_fetch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rp</span><span class="o">.</span><span class="n">can_fetch</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
        <span class="n">test_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_list</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;robotstxt_url&#39;</span><span class="p">,</span> <span class="n">robotstxt_url</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s1">&#39;user_agent&#39;</span><span class="p">,</span> <span class="s1">&#39;url_path&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Elias Dabbas

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>