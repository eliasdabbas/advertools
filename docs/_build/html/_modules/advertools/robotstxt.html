<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>advertools.robotstxt &mdash;  Python</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sphinx-thebe.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
        <script async="async" src="../../_static/sphinx-thebe.js"></script>
        <script async="async" src="https://unpkg.com/thebe@0.8.2/lib/index.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> advertools
          </a>
              <div class="version">
                0.14.0a8
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../readme.html">About advertools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.cli.cli.html">Command Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SEO</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.robotstxt.html">robots.txt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.spider.html">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.code_recipes.spider_strategies.html">Crawl Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.header_spider.html">Crawl headers (HEAD method only)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.logs.html">Log File Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.logs.html#parse-and-analyze-crawl-logs-in-a-dataframe">Parse and Analyze Crawl Logs in a Dataframe</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.reverse_dns_lookup.html">Reverse DNS Lookup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.knowledge_graph.html">Google's Knowledge Graph</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.urlytics.html">URL Structure Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../include_changelog.html">Index &amp; Change Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">advertools</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../index.html">Module code</a> &raquo;</li>
      <li>advertools.robotstxt</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for advertools.robotstxt</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">.. _robotstxt:</span>

<span class="sd">ðŸ¤– Analyze and Test robots.txt Files on a Large Scale</span>
<span class="sd">=====================================================</span>

<span class="sd">Even though they are tiny in size, robots.txt files contain potent instructions</span>
<span class="sd">that can block major sections of your site, which is what they are supposed to</span>
<span class="sd">do. Only sometimes you might make the mistake of blocking the wrong section.</span>

<span class="sd">So it is very important to check if certain pages (or groups of pages) are</span>
<span class="sd">blocked for a certain user-agent by a certain robots.txt file. Ideally, you</span>
<span class="sd">would want to run the same check for all possible user-agents. Even more</span>
<span class="sd">ideally, you want to be able to run the check for a large number of pages with</span>
<span class="sd">every possible combination with user-agents.</span>

<span class="sd">To get the robots.txt file into an easily readable format, you can use the</span>
<span class="sd">:func:`robotstxt_to_df` function to get it in a DataFrame.</span>

<span class="sd">.. thebe-button::</span>
<span class="sd">    Run this code</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    import advertools as adv</span>

<span class="sd">    amazon = adv.robotstxt_to_df(&#39;https://www.amazon.com/robots.txt&#39;)</span>
<span class="sd">    amazon</span>

<span class="sd">====  ===========  =================================  ==================================  =========================  =================================  ================================</span>
<span class="sd">  ..  directive    content                            etag                                robotstxt_last_modified    robotstxt_url                      download_date</span>
<span class="sd">====  ===========  =================================  ==================================  =========================  =================================  ================================</span>
<span class="sd">   0  User-agent   \*                                 &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd">   1  Disallow     /exec/obidos/account-access-login  &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd">   2  Disallow     /exec/obidos/change-style          &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd">   3  Disallow     /exec/obidos/flex-sign-in          &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd">   4  Disallow     /exec/obidos/handle-buy-box        &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd"> ...  ...          ...                                ...                                 ...                        ...                                ...</span>
<span class="sd"> 146  Disallow     /hp/video/mystuff                  &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd"> 147  Disallow     /gp/video/profiles                 &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd"> 148  Disallow     /hp/video/profiles                 &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd"> 149  User-agent   EtaoSpider                         &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd"> 150  Disallow     /                                  &quot;a850165d925db701988daf7ead7492d3&quot;  2021-10-28 17:51:39+00:00  https://www.amazon.com/robots.txt  2022-02-11 19:33:03.200689+00:00</span>
<span class="sd">====  ===========  =================================  ==================================  =========================  =================================  ================================</span>

<span class="sd">The returned DataFrame contains columns for directives, their content, the URL</span>
<span class="sd">of the robots.txt file, as well as the date it was downloaded.</span>

<span class="sd">*  `directive`: The main commands. Allow, Disallow, Sitemap, Crawl-delay,</span>
<span class="sd">   User-agent, and so on.</span>
<span class="sd">*  `content`: The details of each of the directives.</span>
<span class="sd">*  `robotstxt_last_modified`: The date when the robots.txt file was last</span>
<span class="sd">   modified, if provided (according the response header Last-modified).</span>
<span class="sd">*  `etag`: The entity tag of the response header, if provided.</span>
<span class="sd">*  `robotstxt_url`: The URL of the robots.txt file.</span>
<span class="sd">*  `download_date`: The date and time when the file was downloaded.</span>

<span class="sd">Alternatively, you can provide a list of robots URLs if you want to download</span>
<span class="sd">them all in one go. This might be interesting if:</span>

<span class="sd">* You are analyzing an industry and want to keep an eye on many different</span>
<span class="sd">  websites.</span>
<span class="sd">* You are analyzing a website with many sub-domains, and want to get all the</span>
<span class="sd">  robots files together.</span>
<span class="sd">* You are trying to understand a company that has many websites under different</span>
<span class="sd">  domains and sub-domains.</span>

<span class="sd">In this case you simply provide a list of URLs instead of a single one.</span>

<span class="sd">.. thebe-button::</span>
<span class="sd">    Run this code</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    robots_urls = [&#39;https://www.google.com/robots.txt&#39;,</span>
<span class="sd">                   &#39;https://twitter.com/robots.txt&#39;,</span>
<span class="sd">                   &#39;https://facebook.com/robots.txt&#39;]</span>

<span class="sd">    googtwfb = adv.robotstxt_to_df(robots_urls)</span>

<span class="sd">    # How many lines does each robots file have?</span>
<span class="sd">    googtwfb.groupby(&#39;robotstxt_url&#39;)[&#39;directive&#39;].count()</span>

<span class="sd">.. code-block::</span>

<span class="sd">    robotstxt_url</span>
<span class="sd">    https://facebook.com/robots.txt      541</span>
<span class="sd">    https://twitter.com/robots.txt       108</span>
<span class="sd">    https://www.google.com/robots.txt    289</span>
<span class="sd">    Name: directive, dtype: int64</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    # Display the first five rows of each of the robots files:</span>
<span class="sd">    googtwfb.groupby(&#39;robotstxt_url&#39;).head()</span>

<span class="sd">====  ===========  ===================================================================  =========================  =================================  ================================</span>
<span class="sd">  ..  directive    content                                                              robotstxt_last_modified    robotstxt_url                      download_date</span>
<span class="sd">====  ===========  ===================================================================  =========================  =================================  ================================</span>
<span class="sd">   0  User-agent   \*                                                                   2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00</span>
<span class="sd">   1  Disallow     /search                                                              2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00</span>
<span class="sd">   2  Allow        /search/about                                                        2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00</span>
<span class="sd">   3  Allow        /search/static                                                       2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00</span>
<span class="sd">   4  Allow        /search/howsearchworks                                               2022-02-07 22:30:00+00:00  https://www.google.com/robots.txt  2022-02-11 19:52:13.375724+00:00</span>
<span class="sd"> 289  comment      Google Search Engine Robot                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00</span>
<span class="sd"> 290  comment      ==========================                                           NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00</span>
<span class="sd"> 291  User-agent   Googlebot                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00</span>
<span class="sd"> 292  Allow        /?_escaped_fragment_                                                 NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00</span>
<span class="sd"> 293  Allow        /\*?lang=                                                            NaT                        https://twitter.com/robots.txt     2022-02-11 19:52:13.461815+00:00</span>
<span class="sd"> 397  comment      Notice: Collection of data on Facebook through automated means is    NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00</span>
<span class="sd"> 398  comment      prohibited unless you have express written permission from Facebook  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00</span>
<span class="sd"> 399  comment      and may only be conducted for the limited purpose contained in said  NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00</span>
<span class="sd"> 400  comment      permission.                                                          NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00</span>
<span class="sd"> 401  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        NaT                        https://facebook.com/robots.txt    2022-02-11 19:52:13.474456+00:00</span>
<span class="sd">====  ===========  ===================================================================  =========================  =================================  ================================</span>

<span class="sd">Bulk ``robots.txt`` Tester</span>
<span class="sd">--------------------------</span>

<span class="sd">This tester is designed to work on a large scale. The :func:`robotstxt_test`</span>
<span class="sd">function runs a test for a given robots.txt file, checking which of the</span>
<span class="sd">provided user-agents can fetch which of the provided URLs, paths, or patterns.</span>

<span class="sd">.. code-block::</span>

<span class="sd">    robotstxt_test(&#39;https://www.example.com/robots.txt&#39;,</span>
<span class="sd">                   useragents=[&#39;Googlebot&#39;, &#39;baiduspider&#39;, &#39;Bingbot&#39;]</span>
<span class="sd">                   urls=[&#39;/&#39;, &#39;/hello&#39;, &#39;/some-page.html&#39;]])</span>

<span class="sd">As a result, you get a DataFrame with a row for each combination of</span>
<span class="sd">(user-agent, URL) indicating whether or not that particular user-agent can</span>
<span class="sd">fetch the given URL.</span>

<span class="sd">Some reasons why you might want to do that:</span>

<span class="sd">* SEO Audits: Especially for large websites with many URL patterns, and many</span>
<span class="sd">  rules for different user-agents.</span>
<span class="sd">* Developer or site owner about to make large changes</span>
<span class="sd">* Interest in strategies of certain companies</span>

<span class="sd">User-agents</span>
<span class="sd">-----------</span>

<span class="sd">In reality there are only two groups of user-agents that you need to worry</span>
<span class="sd">about:</span>

<span class="sd">* User-agents listed in the robots.txt file: For each one of those you need to</span>
<span class="sd">  check whether or not they are blocked from fetching a certain URL</span>
<span class="sd">  (or pattern).</span>
<span class="sd">* ``*`` all other user-agents: The ``*`` includes all other user-agents, so</span>
<span class="sd">  checking the rules that apply to it should take care of the rest.</span>

<span class="sd">robots.txt Testing Approach</span>
<span class="sd">---------------------------</span>

<span class="sd">1. Get the robots.txt file that you are interested in</span>
<span class="sd">2. Extract the user-agents from it</span>
<span class="sd">3. Specify the URLs you are interested in testing</span>
<span class="sd">4. Run the :func:`robotstxt_test` function</span>

<span class="sd">.. thebe-button::</span>
<span class="sd">    Run this code</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    fb_robots = adv.robotstxt_to_df(&#39;https://www.facebook.com/robots.txt&#39;)</span>
<span class="sd">    fb_robots</span>

<span class="sd">====  ===========  ===================================================================  ===================================  ================================</span>
<span class="sd">  ..  directive    content                                                              robotstxt_url                        download_date</span>
<span class="sd">====  ===========  ===================================================================  ===================================  ================================</span>
<span class="sd">   0  comment      Notice: Collection of data on Facebook through automated means is    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd">   1  comment      prohibited unless you have express written permission from Facebook  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd">   2  comment      and may only be conducted for the limited purpose contained in said  https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd">   3  comment      permission.                                                          https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd">   4  comment      See: http://www.facebook.com/apps/site_scraping_tos_terms.php        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd"> ...  ...          ...                                                                  ...                                  ...                           </span>
<span class="sd"> 536  Allow        /ajax/pagelet/generic.php/PagePostsSectionPagelet                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd"> 537  Allow        /careers/                                                            https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd"> 538  Allow        /safetycheck/                                                        https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd"> 539  User-agent   *                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd"> 540  Disallow     /                                                                    https://www.facebook.com/robots.txt  2022-02-12 00:48:58.951053+00:00</span>
<span class="sd">====  ===========  ===================================================================  ===================================  ================================</span>


<span class="sd">Now that we have downloaded the file, we can easily extract the list of</span>
<span class="sd">user-agents that it contains.</span>

<span class="sd">.. thebe-button::</span>
<span class="sd">    Run this code</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    fb_useragents = (fb_robots</span>
<span class="sd">                     [fb_robots[&#39;directive&#39;]==&#39;User-agent&#39;]</span>
<span class="sd">                     [&#39;content&#39;].drop_duplicates()</span>
<span class="sd">                    .tolist())</span>
<span class="sd">    fb_useragents</span>

<span class="sd">.. code-block::</span>

<span class="sd">    [&#39;Applebot&#39;,</span>
<span class="sd">     &#39;baiduspider&#39;,</span>
<span class="sd">     &#39;Bingbot&#39;,</span>
<span class="sd">     &#39;Discordbot&#39;,</span>
<span class="sd">     &#39;facebookexternalhit&#39;,</span>
<span class="sd">     &#39;Googlebot&#39;,</span>
<span class="sd">     &#39;Googlebot-Image&#39;,</span>
<span class="sd">     &#39;ia_archiver&#39;,</span>
<span class="sd">     &#39;LinkedInBot&#39;,</span>
<span class="sd">     &#39;msnbot&#39;,</span>
<span class="sd">     &#39;Naverbot&#39;,</span>
<span class="sd">     &#39;Pinterestbot&#39;,</span>
<span class="sd">     &#39;seznambot&#39;,</span>
<span class="sd">     &#39;Slurp&#39;,</span>
<span class="sd">     &#39;teoma&#39;,</span>
<span class="sd">     &#39;TelegramBot&#39;,</span>
<span class="sd">     &#39;Twitterbot&#39;,</span>
<span class="sd">     &#39;Yandex&#39;,</span>
<span class="sd">     &#39;Yeti&#39;,</span>
<span class="sd">     &#39;*&#39;]</span>

<span class="sd">Quite a long list!</span>

<span class="sd">As a small and quick test, I&#39;m interested in checking the home page, a random</span>
<span class="sd">profile page (/bbc), groups and hashtags pages.</span>

<span class="sd">.. thebe-button::</span>
<span class="sd">    Run this code</span>

<span class="sd">.. code-block::</span>
<span class="sd">    :class: thebe, thebe-init</span>

<span class="sd">    urls_to_test = [&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;]</span>
<span class="sd">    fb_test = robotstxt_test(&#39;https://www.facebook.com/robots.txt&#39;,</span>
<span class="sd">                             fb_useragents, urls_to_test)</span>
<span class="sd">    fb_test</span>

<span class="sd">====  ===================================  ============  ==========  ===========</span>
<span class="sd">  ..  robotstxt_url                        user_agent    url_path    can_fetch</span>
<span class="sd">====  ===================================  ============  ==========  ===========</span>
<span class="sd">   0  https://www.facebook.com/robots.txt  \*            /           False</span>
<span class="sd">   1  https://www.facebook.com/robots.txt  \*            /bbc        False</span>
<span class="sd">   2  https://www.facebook.com/robots.txt  \*            /groups     False</span>
<span class="sd">   3  https://www.facebook.com/robots.txt  \*            /hashtag/   False</span>
<span class="sd">   4  https://www.facebook.com/robots.txt  Applebot      /           True</span>
<span class="sd">  ..                                  ...           ...         ...          ...</span>
<span class="sd">  75  https://www.facebook.com/robots.txt  seznambot     /hashtag/   True</span>
<span class="sd">  76  https://www.facebook.com/robots.txt  teoma         /           True</span>
<span class="sd">  77  https://www.facebook.com/robots.txt  teoma         /bbc        True</span>
<span class="sd">  78  https://www.facebook.com/robots.txt  teoma         /groups     True</span>
<span class="sd">  79  https://www.facebook.com/robots.txt  teoma         /hashtag/   True</span>
<span class="sd">====  ===================================  ============  ==========  ===========</span>

<span class="sd">For twenty user-agents and four URLs each, we received a total of eighty test</span>
<span class="sd">results. You can immediately see that all user-agents not listed (denoted by</span>
<span class="sd">`*` are not allowed to fetch any of the provided URLs).</span>

<span class="sd">Let&#39;s see who is and who is not allowed to fetch the home page.</span>

<span class="sd">.. code-block::</span>

<span class="sd">    fb_test.query(&#39;url_path== &quot;/&quot;&#39;)</span>

<span class="sd">====  ===================================  ===================  ==========  ===========</span>
<span class="sd">  ..  robotstxt_url                        user_agent           url_path    can_fetch</span>
<span class="sd">====  ===================================  ===================  ==========  ===========</span>
<span class="sd">   0  https://www.facebook.com/robots.txt  \*                   /           False</span>
<span class="sd">   4  https://www.facebook.com/robots.txt  Applebot             /           True</span>
<span class="sd">   8  https://www.facebook.com/robots.txt  Bingbot              /           True</span>
<span class="sd">  12  https://www.facebook.com/robots.txt  Discordbot           /           False</span>
<span class="sd">  16  https://www.facebook.com/robots.txt  Googlebot            /           True</span>
<span class="sd">  20  https://www.facebook.com/robots.txt  Googlebot-Image      /           True</span>
<span class="sd">  24  https://www.facebook.com/robots.txt  LinkedInBot          /           False</span>
<span class="sd">  28  https://www.facebook.com/robots.txt  Naverbot             /           True</span>
<span class="sd">  32  https://www.facebook.com/robots.txt  Pinterestbot         /           False</span>
<span class="sd">  36  https://www.facebook.com/robots.txt  Slurp                /           True</span>
<span class="sd">  40  https://www.facebook.com/robots.txt  TelegramBot          /           False</span>
<span class="sd">  44  https://www.facebook.com/robots.txt  Twitterbot           /           True</span>
<span class="sd">  48  https://www.facebook.com/robots.txt  Yandex               /           True</span>
<span class="sd">  52  https://www.facebook.com/robots.txt  Yeti                 /           True</span>
<span class="sd">  56  https://www.facebook.com/robots.txt  baiduspider          /           True</span>
<span class="sd">  60  https://www.facebook.com/robots.txt  facebookexternalhit  /           False</span>
<span class="sd">  64  https://www.facebook.com/robots.txt  ia_archiver          /           False</span>
<span class="sd">  68  https://www.facebook.com/robots.txt  msnbot               /           True</span>
<span class="sd">  72  https://www.facebook.com/robots.txt  seznambot            /           True</span>
<span class="sd">  76  https://www.facebook.com/robots.txt  teoma                /           True</span>
<span class="sd">====  ===================================  ===================  ==========  ===========</span>

<span class="sd">I&#39;ll leave it to you to figure out why LinkedIn and Pinterest are not allowed</span>
<span class="sd">to crawl the home page but Google and Apple are, because I have no clue!</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;robotstxt_to_df&#39;</span><span class="p">,</span> <span class="s1">&#39;robotstxt_test&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">gzip</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">concurrent</span> <span class="kn">import</span> <span class="n">futures</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">urllib.request</span> <span class="kn">import</span> <span class="n">Request</span><span class="p">,</span> <span class="n">urlopen</span>

<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">protego</span> <span class="kn">import</span> <span class="n">Protego</span>

<span class="kn">from</span> <span class="nn">advertools</span> <span class="kn">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">version</span>

<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;advertools-&#39;</span> <span class="o">+</span> <span class="n">version</span><span class="p">}</span>

<span class="n">gzip_start_bytes</span> <span class="o">=</span> <span class="sa">b</span><span class="s1">&#39;</span><span class="se">\x1f\x8b</span><span class="s1">&#39;</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>


<div class="viewcode-block" id="robotstxt_to_df"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_to_df">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_to_df</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Download the contents of ``robotstxt_url`` into a DataFrame</span>

<span class="sd">    You can also use it to download multiple robots files by passing a list of</span>
<span class="sd">    URLs.</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df(&#39;https://www.twitter.com/robots.txt&#39;)</span>
<span class="sd">         directive content   	                 robotstxt_url	                   download_date</span>
<span class="sd">    0	User-agent	     *	https://www.twitter.com/robots.txt	2020-09-27 21:57:23.702814+00:00</span>
<span class="sd">    1	  Disallow	     /	https://www.twitter.com/robots.txt	2020-09-27 21:57:23.702814+00:00</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df([&#39;https://www.google.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://www.twitter.com/robots.txt&#39;])</span>
<span class="sd">           directive	                             content	    robotstxt_last_modified	                       robotstxt_url	                     download_date</span>
<span class="sd">    0	  User-agent	                                   *	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    1	    Disallow	                             /search	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    2	       Allow	                       /search/about	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    3	       Allow	                      /search/static	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    4	       Allow	              /search/howsearchworks	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    283	  User-agent	                 facebookexternalhit	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    284	       Allow	                             /imgres	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    285	     Sitemap	  https://www.google.com/sitemap.xml	  2021-01-11 21:00:00+00:00	   https://www.google.com/robots.txt	  2021-01-16 14:08:50.087985+00:00</span>
<span class="sd">    286	  User-agent	                                   *	                        NaT	  https://www.twitter.com/robots.txt	  2021-01-16 14:08:50.468588+00:00</span>
<span class="sd">    287	    Disallow	                                   /	                        NaT	  https://www.twitter.com/robots.txt	  2021-01-16 14:08:50.468588+00:00</span>

<span class="sd">    For research purposes and if you want to download more than ~500 files, you</span>
<span class="sd">    might want to use ``output_file`` to save results as they are downloaded.</span>
<span class="sd">    The file extension should be &quot;.jl&quot;, and robots files are appended to that</span>
<span class="sd">    file as soon as they are downloaded, in case you lose your connection, or</span>
<span class="sd">    maybe your patience!</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_to_df([&#39;https://example.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://example.com/robots.txt&#39;,</span>
<span class="sd">    ...                  &#39;https://example.com/robots.txt&#39;],</span>
<span class="sd">    ...                 output_file=&#39;robots_output_file.jl&#39;)</span>

<span class="sd">    To open the file as a DataFrame:</span>

<span class="sd">    &gt;&gt;&gt; import pandas as pd</span>
<span class="sd">    &gt;&gt;&gt; robotsfiles_df = pd.read_json(&#39;robots_output_file.jl&#39;, lines=True)</span>

<span class="sd">    :param url robotstxt_url: One or more URLs of the robots.txt file(s)</span>
<span class="sd">    :param str output_file: Optional file path to save the robots.txt files,</span>
<span class="sd">                            mainly useful for downloading &gt; 500 files. The</span>
<span class="sd">                            files are appended as soon as they are downloaded.</span>
<span class="sd">                            Only &quot;.jl&quot; extensions are supported.</span>

<span class="sd">    :returns DataFrame robotstxt_df: A DataFrame containing directives, their</span>
<span class="sd">                                     content, the URL and time of download</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">output_file</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.jl&#39;</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please specify a file with a `.jl` extension.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">,</span> <span class="nb">set</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)):</span>
        <span class="k">return</span> <span class="n">_robots_multi</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">output_file</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="s1">&#39;Getting: &#39;</span> <span class="o">+</span> <span class="n">robotstxt_url</span><span class="p">)</span>
            <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">),</span>
                                  <span class="n">timeout</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
            <span class="n">robots_read</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">robots_read</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">gzip_start_bytes</span><span class="p">):</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">gzip</span><span class="o">.</span><span class="n">decompress</span><span class="p">(</span><span class="n">robots_read</span><span class="p">)</span>
                <span class="n">robots_text</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">robots_text</span> <span class="o">=</span> <span class="n">robots_read</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8-sig&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
            <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_text</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
                    <span class="k">if</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">):</span>
                        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="s1">&#39;comment&#39;</span><span class="p">,</span>
                                      <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;#&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">())])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">split</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">split</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()])</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">lines</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;directive&#39;</span><span class="p">,</span> <span class="s1">&#39;content&#39;</span><span class="p">])</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">etag_lastmod</span> <span class="o">=</span> <span class="p">{</span><span class="n">header</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">):</span> <span class="n">val</span>
                                <span class="k">for</span> <span class="n">header</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">getheaders</span><span class="p">()</span>
                                <span class="k">if</span> <span class="n">header</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;etag&#39;</span><span class="p">,</span> <span class="s1">&#39;last-modified&#39;</span><span class="p">]}</span>
                <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="o">**</span><span class="n">etag_lastmod</span><span class="p">)</span>
                <span class="k">if</span> <span class="s1">&#39;last_modified&#39;</span> <span class="ow">in</span> <span class="n">df</span><span class="p">:</span>
                    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_last_modified&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;last_modified&#39;</span><span class="p">])</span>
                    <span class="k">del</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;last_modified&#39;</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;errors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)]})</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">robots_open</span><span class="o">.</span><span class="n">url</span><span class="p">]</span> <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span> <span class="k">else</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">url</span>
        <span class="k">except</span> <span class="ne">UnboundLocalError</span><span class="p">:</span>
            <span class="n">df</span><span class="p">[</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">robotstxt_url</span><span class="p">]</span> <span class="k">if</span> <span class="n">df</span><span class="o">.</span><span class="n">empty</span> <span class="k">else</span> <span class="n">robotstxt_url</span>
        <span class="n">df</span><span class="p">[</span><span class="s1">&#39;download_date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Timestamp</span><span class="o">.</span><span class="n">now</span><span class="p">(</span><span class="n">tz</span><span class="o">=</span><span class="s1">&#39;UTC&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s1">&#39;records&#39;</span><span class="p">,</span>
                                      <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                      <span class="n">date_format</span><span class="o">=</span><span class="s1">&#39;iso&#39;</span><span class="p">))</span>
                <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">df</span></div>


<span class="k">def</span> <span class="nf">_robots_multi</span><span class="p">(</span><span class="n">robots_url_list</span><span class="p">,</span> <span class="n">output_file</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
        <span class="n">to_do</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">robotsurl</span> <span class="ow">in</span> <span class="n">robots_url_list</span><span class="p">:</span>
            <span class="n">future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">robotstxt_to_df</span><span class="p">,</span> <span class="n">robotsurl</span><span class="p">)</span>
            <span class="n">to_do</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">future</span><span class="p">)</span>
        <span class="n">done_iter</span> <span class="o">=</span> <span class="n">futures</span><span class="o">.</span><span class="n">as_completed</span><span class="p">(</span><span class="n">to_do</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">done_iter</span><span class="p">:</span>
            <span class="n">future_result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">output_file</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
                    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">future_result</span><span class="o">.</span><span class="n">to_json</span><span class="p">(</span><span class="n">orient</span><span class="o">=</span><span class="s1">&#39;records&#39;</span><span class="p">,</span>
                                                     <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                     <span class="n">date_format</span><span class="o">=</span><span class="s1">&#39;iso&#39;</span><span class="p">))</span>
                    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">final_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">final_df</span><span class="p">,</span> <span class="n">future_result</span><span class="p">],</span>
                                     <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">final_df</span>


<div class="viewcode-block" id="robotstxt_test"><a class="viewcode-back" href="../../advertools.robotstxt.html#advertools.robotstxt.robotstxt_test">[docs]</a><span class="k">def</span> <span class="nf">robotstxt_test</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">,</span> <span class="n">urls</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Given a :attr:`robotstxt_url` check which of the :attr:`user_agents` is</span>
<span class="sd">    allowed to fetch which of the :attr:`urls`.</span>

<span class="sd">    All the combinations of :attr:`user_agents` and :attr:`urls` will be</span>
<span class="sd">    checked and the results returned in one DataFrame.</span>

<span class="sd">    &gt;&gt;&gt; robotstxt_test(&#39;https://facebook.com/robots.txt&#39;,</span>
<span class="sd">    ...                user_agents=[&#39;*&#39;, &#39;Googlebot&#39;, &#39;Applebot&#39;],</span>
<span class="sd">    ...                urls=[&#39;/&#39;, &#39;/bbc&#39;, &#39;/groups&#39;, &#39;/hashtag/&#39;])</span>
<span class="sd">                          robotstxt_url user_agent   url_path  can_fetch</span>
<span class="sd">    0   https://facebook.com/robots.txt          *          /      False</span>
<span class="sd">    1   https://facebook.com/robots.txt          *       /bbc      False</span>
<span class="sd">    2   https://facebook.com/robots.txt          *    /groups      False</span>
<span class="sd">    3   https://facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="sd">    4   https://facebook.com/robots.txt   Applebot          /       True</span>
<span class="sd">    5   https://facebook.com/robots.txt   Applebot       /bbc       True</span>
<span class="sd">    6   https://facebook.com/robots.txt   Applebot    /groups       True</span>
<span class="sd">    7   https://facebook.com/robots.txt   Applebot  /hashtag/      False</span>
<span class="sd">    8   https://facebook.com/robots.txt  Googlebot          /       True</span>
<span class="sd">    9   https://facebook.com/robots.txt  Googlebot       /bbc       True</span>
<span class="sd">    10  https://facebook.com/robots.txt  Googlebot    /groups       True</span>
<span class="sd">    11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False</span>

<span class="sd">    :param url robotstxt_url: The URL of robotx.txt file</span>
<span class="sd">    :param str,list user_agents: One or more user agents</span>
<span class="sd">    :param str,list urls: One or more paths (relative) or URLs (absolute) to</span>
<span class="sd">                           check</span>
<span class="sd">    :return DataFrame robotstxt_test_df:</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">robotstxt_url</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;/robots.txt&#39;</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Please make sure you enter a valid robots.txt URL&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">user_agents</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">user_agents</span> <span class="o">=</span> <span class="p">[</span><span class="n">user_agents</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="n">urls</span><span class="p">]</span>
    <span class="n">robots_open</span> <span class="o">=</span> <span class="n">urlopen</span><span class="p">(</span><span class="n">Request</span><span class="p">(</span><span class="n">robotstxt_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">))</span>
    <span class="n">robots_bytes</span> <span class="o">=</span> <span class="n">robots_open</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
    <span class="n">robots_text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">robots_bytes</span><span class="p">)</span>
    <span class="n">rp</span> <span class="o">=</span> <span class="n">Protego</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">robots_text</span><span class="p">)</span>

    <span class="n">test_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">path</span><span class="p">,</span> <span class="n">agent</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">user_agents</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;user_agent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;url_path&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">d</span><span class="p">[</span><span class="s1">&#39;can_fetch&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rp</span><span class="o">.</span><span class="n">can_fetch</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
        <span class="n">test_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_list</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;robotstxt_url&#39;</span><span class="p">,</span> <span class="n">robotstxt_url</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s1">&#39;user_agent&#39;</span><span class="p">,</span> <span class="s1">&#39;url_path&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Elias Dabbas.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>