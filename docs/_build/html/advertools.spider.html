

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>üï∑ Python SEO Crawler / Spider &mdash;  Python</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Import Search Engine Results Pages (SERPs) for Google and YouTube" href="advertools.serp.html" />
    <link rel="prev" title="Download, Parse, and Analyze XML Sitemaps" href="advertools.sitemaps.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> advertools
          

          
          </a>

          
            
            
              <div class="version">
                0.10.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">About advertools</a></li>
</ul>
<p class="caption"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption"><span class="caption-text">SEO</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="advertools.robotstxt.html">robots.txt</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">SEO Spider / Crawler</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#discovery-crawling-approach">Discovery Crawling Approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extracted-on-page-seo-elements">Extracted On-Page SEO Elements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-determined-crawling-approach-list-mode">Pre-Determined Crawling Approach (List Mode)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#serp-data">SERP Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#news-articles">News Articles</a></li>
<li class="toctree-l3"><a class="reference internal" href="#google-analytics-google-search-console">Google Analytics / Google Search Console</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#css-and-xpath-selectors">CSS and XPath Selectors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#spider-custom-settings-and-additional-functionality">Spider Custom Settings and Additional Functionality</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.knowledge_graph.html">Google's Knowledge Graph</a></li>
</ul>
<p class="caption"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.urlytics.html">URL Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="include_changelog.html">Index &amp; Change Log</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">advertools</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>üï∑ Python SEO Crawler / Spider</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/advertools.spider.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <span class="target" id="module-advertools.spider"></span><div class="section" id="python-seo-crawler-spider">
<span id="crawl"></span><h1>üï∑ Python SEO Crawler / Spider<a class="headerlink" href="#python-seo-crawler-spider" title="Permalink to this headline">¬∂</a></h1>
<p>A customizable crawler to analyze SEO and content of pages and websites.</p>
<p>This is provided by the <a class="reference internal" href="#advertools.spider.crawl" title="advertools.spider.crawl"><code class="xref py py-func docutils literal notranslate"><span class="pre">crawl()</span></code></a> function which is customized for SEO and
content analysis usage, and is highly configurable. The crawler uses
<a class="reference external" href="https://scrapy.org/">Scrapy</a> so you get all the power that it provides in
terms of performance, speed, as well as flexibility and customization.</p>
<p>There are two main approaches to crawl:</p>
<ol class="arabic simple">
<li><p><strong>Discovery:</strong> You know the website to crawl, so you provide a <code class="docutils literal notranslate"><span class="pre">url_list</span></code>
(one or more URLs), and you want the crawler to go through the whole
website(s) by following all available links.</p></li>
<li><p><strong>Pre-determined a.k.a ‚Äúlist mode‚Äù:</strong> You have a known set of URLs that you
want to crawl and analyze, without following links or discovering new URLs.</p></li>
</ol>
<div class="section" id="discovery-crawling-approach">
<h2>Discovery Crawling Approach<a class="headerlink" href="#discovery-crawling-approach" title="Permalink to this headline">¬∂</a></h2>
<p>The simplest way to use the function is to provide a list of one or more URLs
and the crawler will go through all of the reachable pages.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;https://example.com&#39;</span><span class="p">,</span> <span class="s1">&#39;my_output_file.jl&#39;</span><span class="p">,</span> <span class="n">follow_links</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>That‚Äôs it! To open the file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s1">&#39;my_output_file.jl&#39;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<p>What this does:</p>
<ul class="simple">
<li><p>Check the site‚Äôs robots.txt file and get the crawl rules, which means that
your crawl will be affected by these rules and the user agent you are using.
Check the details below on how to change settings and user agents to control
this.</p></li>
<li><p>Starting with the provided URL(s) go through all links and parse pages.</p></li>
<li><p>For each URL extract the most important SEO elements.</p></li>
<li><p>Save them to <code class="docutils literal notranslate"><span class="pre">my_output_file.jl</span></code>.</p></li>
<li><p>The column headers of the output file (once you import it as a DataFrame)
would be the names of the elements (title, h1, h2, etc.).</p></li>
</ul>
<p>Jsonlines is the supported output format because of its flexibility in allowing
different values for different scraped pages, and appending indepentent items
to the output files.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the crawler parses pages it saves the data to the specified file by
appending, and not overwriting. Otherwise it would have to store all the
data in memory, which might crash your computer. A good practice is to have
a separate <code class="docutils literal notranslate"><span class="pre">output_file</span></code> for every crawl with a descriptive name
<cite>sitename_crawl_YYYY_MM_DD.jl</cite> for example. If you use the same file you
will probably get duplicate data in the same file.</p>
</div>
</div>
<div class="section" id="extracted-on-page-seo-elements">
<h2>Extracted On-Page SEO Elements<a class="headerlink" href="#extracted-on-page-seo-elements" title="Permalink to this headline">¬∂</a></h2>
<p>The names of these elements become the headers (column names) of the
<code class="docutils literal notranslate"><span class="pre">output_file</span></code>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 22%" />
<col style="width: 78%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Element</p></th>
<th class="head"><p>Remarks</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>url</p></td>
<td><p>The URL requested</p></td>
</tr>
<tr class="row-odd"><td><p>url_redirected_to</p></td>
<td><p>The actual URL that was parsed, usually but not always the
same as <cite>url</cite></p></td>
</tr>
<tr class="row-even"><td><p>title</p></td>
<td><p>The &lt;title&gt; tag(s)</p></td>
</tr>
<tr class="row-odd"><td><p>meta_desc</p></td>
<td><p>Meta description</p></td>
</tr>
<tr class="row-even"><td><p>canonical</p></td>
<td><p>The canonical tag if available</p></td>
</tr>
<tr class="row-odd"><td><p>alt_href</p></td>
<td><p>The <cite>href</cite> attribute of rel=alternate tags</p></td>
</tr>
<tr class="row-even"><td><p>alt_hreflang</p></td>
<td><p>The language codes of the alternate links</p></td>
</tr>
<tr class="row-odd"><td><p>og:*</p></td>
<td><p>Open Graph data</p></td>
</tr>
<tr class="row-even"><td><p>twitter:*</p></td>
<td><p>Twitter card data</p></td>
</tr>
<tr class="row-odd"><td><p>jsonld_*</p></td>
<td><p>JSON-LD data if available. In case multiple snippets occur,
the respective column names will include a number to
distinguish them, <cite>jsonld_1_{item_a}, jsonld_1_{item_b}</cite>,
etc. Note that the first snippet will not contain a number,
so the numbering starts with ‚Äú1‚Äù, starting from the second
snippet. The same applies to OG and Twitter cards.</p></td>
</tr>
<tr class="row-even"><td><p>h1</p></td>
<td><p><cite>&lt;h1&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-odd"><td><p>h2</p></td>
<td><p><cite>&lt;h2&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-even"><td><p>h3</p></td>
<td><p><cite>&lt;h3&gt;</cite> tag(s)</p></td>
</tr>
<tr class="row-odd"><td><p>body_text</p></td>
<td><p>The text in the &lt;p&gt; tags</p></td>
</tr>
<tr class="row-even"><td><p>size</p></td>
<td><p>The page size in bytes</p></td>
</tr>
<tr class="row-odd"><td><p>resp_meta_*</p></td>
<td><p>Several metadata for the response download_latency, timeout
etc.</p></td>
</tr>
<tr class="row-even"><td><p>status</p></td>
<td><p>Response status (200, 301, 302, 404, etc.)</p></td>
</tr>
<tr class="row-odd"><td><p>links_url</p></td>
<td><p>The URLs of the links on the page</p></td>
</tr>
<tr class="row-even"><td><p>links_text</p></td>
<td><p>The link text (anchor text)</p></td>
</tr>
<tr class="row-odd"><td><p>links_fragment</p></td>
<td><p>The fragment part of the link (#fragment)</p></td>
</tr>
<tr class="row-even"><td><p>links_nofollow</p></td>
<td><p>Boolean, whether or not the link is a nofllow link. Note that
this only tells if the link itself contains a rel=‚Äùnofollow‚Äù
attribute. The page might indicate ‚Äúnofollow‚Äù using meta
robots or X-Robots-Tag, which you have to check separately.</p></td>
</tr>
<tr class="row-odd"><td><p>img_src</p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">src</span></code> attribute of images</p></td>
</tr>
<tr class="row-even"><td><p>img_alt</p></td>
<td><p>The <code class="docutils literal notranslate"><span class="pre">alt</span></code> attribute if available or an empty string</p></td>
</tr>
<tr class="row-odd"><td><p>page_depth</p></td>
<td><p>The depth of the crawled page</p></td>
</tr>
<tr class="row-even"><td><p>ip_address</p></td>
<td><p>IP address</p></td>
</tr>
<tr class="row-odd"><td><p>crawl_time</p></td>
<td><p>Date and time the page was crawled</p></td>
</tr>
<tr class="row-even"><td><p>resp_headers_*</p></td>
<td><p>All available response headers (last modified, server, etc.)</p></td>
</tr>
<tr class="row-odd"><td><p>request_headers_*</p></td>
<td><p>All available request headers (user-agent, encoding, etc.)</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All elements that may appear multiple times on a page (like header tags, or
images, for example), will be joined with two ‚Äú&#64;‚Äù signs <cite>&#64;&#64;</cite>. For example,
<strong>‚Äúfirst H2 tag&#64;&#64;second H2 tag&#64;&#64;third tag‚Äù</strong> and so on.
Once you open the file, you simply have to split by <cite>&#64;&#64;</cite> to get the
elements as a list.</p>
</div>
<p>Here is a sample file of a crawl of this site (output truncated for
readability):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">site_crawl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s1">&#39;path/to/file.jl&#39;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">site_crawl</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="go">                               url               url_redirected_to                           title                       meta_desc                              h1                              h2                              h3                        body_text  size  download_timeout              download_slot  download_latency  redirect_times  redirect_ttl                   redirect_urls redirect_reasons  depth  status                      links_href                      links_text                         img_src                         img_alt    ip_address           crawl_time              resp_headers_date resp_headers_content-type     resp_headers_last-modified resp_headers_vary    resp_headers_x-ms-request-id resp_headers_x-ms-version resp_headers_x-ms-lease-status resp_headers_x-ms-blob-type resp_headers_access-control-allow-origin   resp_headers_x-served resp_headers_x-backend resp_headers_x-rtd-project resp_headers_x-rtd-version         resp_headers_x-rtd-path  resp_headers_x-rtd-domain resp_headers_x-rtd-version-method resp_headers_x-rtd-project-method resp_headers_strict-transport-security resp_headers_cf-cache-status  resp_headers_age           resp_headers_expires resp_headers_cache-control          resp_headers_expect-ct resp_headers_server   resp_headers_cf-ray      resp_headers_cf-request-id          request_headers_accept request_headers_accept-language      request_headers_user-agent request_headers_accept-encoding          request_headers_cookie</span>
<span class="go">0   https://advertools.readthedocs  https://advertools.readthedocs            advertools ‚Äî  Python  Get productive as an online ma  advertools@@Indices and tables  Online marketing productivity                              NaN   Generate keywords for SEM camp   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN  https://advertools.readthedocs            [302]    NaN     NaN  #@@readme.html@@advertools.kw_  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:35  Thu, 21 May 2020 10:39:35 GMT                 text/html  Wed, 20 May 2020 12:26:23 GMT   Accept-Encoding  720a8581-501e-0043-01a2-2e77d2                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007c                 advertools                     master  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:35 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596daca7dbaa7e9e-BUD  02d86a3cea00007e9edb0cf2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">1   https://advertools.readthedocs  https://advertools.readthedocs            advertools ‚Äî  Python                             NaN                      advertools         Change Log - advertools  0.9.1 (2020-05-19)@@0.9.0 (202   Ability to specify robots.txt    NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  index.html@@readme.html@@adver  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:36  Thu, 21 May 2020 10:39:35 GMT                 text/html  Wed, 20 May 2020 12:26:23 GMT   Accept-Encoding  4f7bea3b-701e-0039-3f44-2f1d9f                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007h                 advertools                     master  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:35 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596daca9bcab7e9e-BUD  02d86a3e0e00007e9edb0d72000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">2   https://advertools.readthedocs  https://advertools.readthedocs            advertools ‚Äî  Python  Get productive as an online ma  advertools@@Indices and tables  Online marketing productivity                              NaN   Generate keywords for SEM camp   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  #@@readme.html@@advertools.kw_  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:36  Thu, 21 May 2020 10:39:35 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  98b729fa-e01e-00bf-24c3-2e494d                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007c                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:35 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596daca9bf26d423-BUD  02d86a3e150000d423322742000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">3   https://advertools.readthedocs  https://advertools.readthedocs    advertools package ‚Äî  Python                             NaN              advertools package     Submodules@@Module contents                             NaN   Top-level package for advertoo   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  index.html@@readme.html@@adver  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:36  Thu, 21 May 2020 10:39:35 GMT                 text/html  Wed, 20 May 2020 12:26:25 GMT   Accept-Encoding  7a28ef3b-801e-00c2-24c3-2ed585                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web000079                 advertools                     master  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:35 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596daca9bddb7ec2-BUD  02d86a3e1300007ec2a808a2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">4   https://advertools.readthedocs  https://advertools.readthedocs   Python Module Index ‚Äî  Python                             NaN             Python Module Index                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  index.html@@readme.html@@adver  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@               _static/minus.png                               -  104.17.32.82  2020-05-21 10:39:36  Thu, 21 May 2020 10:39:35 GMT                 text/html  Wed, 20 May 2020 12:26:23 GMT   Accept-Encoding  75911c9e-201e-00e6-34c3-2e4ccb                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007g                 advertools                     master  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:35 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596daca9b91fd437-BUD  02d86a3e140000d437b81532000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">66  https://advertools.readthedocs  https://advertools.readthedocs  advertools.url_builders ‚Äî  Pyt                             NaN  Source code for advertools.url                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  ../../index.html@@../../readme  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:39  Thu, 21 May 2020 10:39:38 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  d99f2368-c01e-006f-18c3-2ef5ef                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007a                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:38 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596dacbbb8afd437-BUD  02d86a494f0000d437b828b2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">67  https://advertools.readthedocs  https://advertools.readthedocs  advertools.kw_generate ‚Äî  Pyth                             NaN  Source code for advertools.kw_                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  ../../index.html@@../../readme  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:39  Thu, 21 May 2020 10:39:39 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  85855c48-c01e-00ce-13c3-2e3b74                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007g                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:39 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596dacbd980bd423-BUD  02d86a4a7f0000d423323b42000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">68  https://advertools.readthedocs  https://advertools.readthedocs  advertools.ad_from_string ‚Äî  P                             NaN  Source code for advertools.ad_                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  ../../index.html@@../../readme  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:39  Thu, 21 May 2020 10:39:39 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  b0aef497-801e-004a-1647-2f6d5c                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007k                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:39 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596dacbd980cd423-BUD  02d86a4a7f0000d423209db2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">69  https://advertools.readthedocs  https://advertools.readthedocs  advertools.ad_create ‚Äî  Python                             NaN  Source code for advertools.ad_                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  ../../index.html@@../../readme  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:39  Thu, 21 May 2020 10:39:39 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  9dfdd38a-101e-00a1-7ec3-2e93a0                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web00007c                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:39 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596dacbd99847ec2-BUD  02d86a4a7f00007ec2a811f2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
<span class="go">70  https://advertools.readthedocs  https://advertools.readthedocs      advertools.emoji ‚Äî  Python                             NaN  Source code for advertools.emo                             NaN                             NaN            ¬© Copyright 2020, Eli   NaN               NaN  advertools.readthedocs.io               NaN             NaN           NaN                             NaN              NaN    NaN     NaN  ../../index.html@@../../readme  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                             NaN                             NaN  104.17.32.82  2020-05-21 10:39:40  Thu, 21 May 2020 10:39:39 GMT                 text/html  Wed, 20 May 2020 12:26:36 GMT   Accept-Encoding  2ad504a1-101e-000b-03c3-2e454f                2009-09-19                       unlocked                   BlockBlob                                        *  Nginx-Proxito-Sendfile              web000079                 advertools                     latest  /proxito/media/html/advertools  advertools.readthedocs.io                              path                         subdomain         max-age=31536000; includeSubDo                          HIT               NaN  Thu, 21 May 2020 11:39:39 GMT       public, max-age=3600  max-age=604800, report-uri=&quot;ht          cloudflare  596dacbd9fb97e9e-BUD  02d86a4a7f00007e9edb13a2000000  text/html,application/xhtml+xm                              en  Mozilla/5.0 (Windows NT 10.0;                    gzip, deflate  __cfduid=d76b68d148ddec1efd004</span>
</pre></div>
</div>
</div>
<div class="section" id="pre-determined-crawling-approach-list-mode">
<h2>Pre-Determined Crawling Approach (List Mode)<a class="headerlink" href="#pre-determined-crawling-approach-list-mode" title="Permalink to this headline">¬∂</a></h2>
<p>Sometimes you might have a fixed set of URLs for which you want to scrape and
analyze SEO or content performance. Some ideas:</p>
<div class="section" id="serp-data">
<h3>SERP Data<a class="headerlink" href="#serp-data" title="Permalink to this headline">¬∂</a></h3>
<p>Let‚Äôs say you just ran <a class="reference internal" href="advertools.serp.html#serp"><span class="std std-ref">serp_goog</span></a> and got a bunch of top-ranking
pages that you would like to analyze, and see how that relates to their SERP
ranking.</p>
<p>You simply provide the <code class="docutils literal notranslate"><span class="pre">url_list</span></code> parameter and again specify the
<code class="docutils literal notranslate"><span class="pre">output_file</span></code>. This will only crawl the specified URLs, and will not follow
any links.</p>
<p>Now you have the SERP DataFrame, as well as the crawl output file. All you have
to do is to merge them by the URL columns, and end up with a richer dataset</p>
</div>
<div class="section" id="news-articles">
<h3>News Articles<a class="headerlink" href="#news-articles" title="Permalink to this headline">¬∂</a></h3>
<p>You want to follow the latest news of a certain publication, and you extract
their latest news URLs from their news sitemap using
<a class="reference internal" href="advertools.sitemaps.html#sitemaps"><span class="std std-ref">sitemap_to_df</span></a> . You provide those URLs and crawl them only.</p>
</div>
<div class="section" id="google-analytics-google-search-console">
<h3>Google Analytics / Google Search Console<a class="headerlink" href="#google-analytics-google-search-console" title="Permalink to this headline">¬∂</a></h3>
<p>Since they provide reports for URLs, you can also combine them with the ones
crawled and end up with a better perspective. You might be interested in
knowing more about high bounce-rate pages, pages that convert well, pages that
get less traffic than you think they should and so on. You can simply export
those URLs and crawl them.</p>
<p>Any tool that has data about a set of URLs can be used.</p>
<p>Again running the function is as simple as providing a list of URLs, as well as
a filepath where you want the result saved.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="n">url_list</span><span class="p">,</span> <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span> <span class="n">follow_links</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The difference between the two approaches, is the simple parameter
<code class="docutils literal notranslate"><span class="pre">follow_links</span></code>. If you keep it as <code class="docutils literal notranslate"><span class="pre">False</span></code> (the default), the crawler
will only go through the provided URLs. Otherwise, it will discover pages by
following links on pages that it crawls. So how do you make sure that the
crawler doesn‚Äôt try to crawl the whole web when <code class="docutils literal notranslate"><span class="pre">follow_links</span></code> is <cite>True</cite>?
The <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> parameter gives you the ability to control this,
although it is an optional parameter. If you don‚Äôt specify it, then it will
default to only the domains in the <code class="docutils literal notranslate"><span class="pre">url_list</span></code>. It‚Äôs important to note that
you have to set this parameter if you have certain sub-domains that you want to
crawl.</p>
</div>
</div>
<div class="section" id="css-and-xpath-selectors">
<h2>CSS and XPath Selectors<a class="headerlink" href="#css-and-xpath-selectors" title="Permalink to this headline">¬∂</a></h2>
<p>The above approaches are generic, and are useful for an exploratory SEO audit
and the output is helpful for most cases.</p>
<p>But what if you want to extract special elements that are not included in the
default output? This is extremely important, as there are key elements on pages
that you need to additionally extract and analyze. Some examples might be tags,
prices, social media shares, product price or availability, comments, and
pretty much any element on a page that might be of interest to you.</p>
<p>For this you can use two special arguments for CSS and/or XPath selectors. You
simply provide a dictionary <cite>{‚Äòname_1‚Äô: ‚Äòselector_1‚Äô, ‚Äòname_2‚Äô: ‚Äòselector_2‚Äô}</cite>
where the keys become the column names, and the values (selectors) will be
used to extract the required elements.</p>
<p>I mostly rely on <a class="reference external" href="https://selectorgadget.com/">SlectorGadget</a> which is a
really great tool for getting the CSS/XPath selecotrs of required elements.
In some pages it can get really tricky to figure that out. Other resources for
learning more about selectors:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.scrapy.org/en/latest/topics/selectors.html">Scrapy‚Äôs documentaion for selectors</a></p></li>
<li><p><a class="reference external" href="https://www.w3schools.com/cssref/css_selectors.asp">CSS Selector Reference on W3C</a></p></li>
<li><p><a class="reference external" href="https://www.w3schools.com/xml/xpath_intro.asp">XPath tutorial on W3C</a></p></li>
</ul>
<p>Once you have determined the elements that you want to extract and figured out
what their names are going to be, you simply pass them as arguments to
<code class="docutils literal notranslate"><span class="pre">css_selectors</span></code> and/or <code class="docutils literal notranslate"><span class="pre">xpath_selectors</span></code> as dictionaries, as decribed
above.</p>
<p>Let‚Äôs say you want to extract the links in the sidebar of this page. By default
you would get all the links from the page, but you want to put those in the
sidebar in a separate column. It seems that the CSS selector for them is
<cite>.toctree-l1 .internal</cite>, and the XPath equivalent is
<cite>//*[contains(concat( ‚Äù ‚Äú, &#64;class, ‚Äù ‚Äù ), concat( ‚Äù ‚Äú, ‚Äútoctree-l1‚Äù, ‚Äù ‚Äù ))]//*[contains(concat( ‚Äù ‚Äú, &#64;class, ‚Äù ‚Äù ), concat( ‚Äù ‚Äú, ‚Äúinternal‚Äù, ‚Äù ‚Äù ))]</cite>.
Note that this selects the <em>element</em> (the whole link object), which is not
typically what you might be interested in.</p>
<p>So with CSS you need to append <cite>::text</cite> or <cite>::attr(href)</cite> if you want the text
of the links or the <cite>href</cite> attribute respectively. Similarly with XPath, you
will need to append <cite>/text()</cite> or <cite>/&#64;href</cite> to the selector to get the same.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;https://advertools.readthedocs.io/en/master/advertools.spider.html&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="n">css_selectors</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sidebar_links&#39;</span><span class="p">:</span> <span class="s1">&#39;.toctree-l1 .internal::text&#39;</span><span class="p">,</span>
<span class="gp">... </span>                     <span class="s1">&#39;sidebar_links_url&#39;</span><span class="p">:</span> <span class="s1">&#39;.toctree-l1 .internal::attr(href)&#39;</span><span class="p">})</span>
</pre></div>
</div>
<p>Or, instead of <code class="docutils literal notranslate"><span class="pre">css_selectors</span></code> you can add a similar dictionary for the
<code class="docutils literal notranslate"><span class="pre">xpath_selectors</span></code> argument:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;https://advertools.readthedocs.io/en/master/advertools.spider.html&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="n">xpath_selectors</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;sidebar_links&#39;</span><span class="p">:</span> <span class="s1">&#39;//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;toctree-l1&quot;, &quot; &quot; ))]//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;internal&quot;, &quot; &quot; ))]/text()&#39;</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="s1">&#39;sidebar_links_url&#39;</span><span class="p">:</span> <span class="s1">&#39;//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;toctree-l1&quot;, &quot; &quot; ))]//*[contains(concat( &quot; &quot;, @class, &quot; &quot; ), concat( &quot; &quot;, &quot;internal&quot;, &quot; &quot; ))]/@href&#39;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-custom-settings-and-additional-functionality">
<h2>Spider Custom Settings and Additional Functionality<a class="headerlink" href="#spider-custom-settings-and-additional-functionality" title="Permalink to this headline">¬∂</a></h2>
<p>In addition to what you can control regarding the items you can extract, you
can also customize the behaviour of the spider and set rules for crawling so
you can control it even further.</p>
<p>This is provided by the <code class="docutils literal notranslate"><span class="pre">custom_settings</span></code> parameter. It is optional, and
takes a dictionary of settings and their values. Scrapy provides a very large
number of settings, and they are all available through this parameter
(assuming some conditions for some of the settings).</p>
<p>Here are some examples that you might find interesting:</p>
<ul class="simple">
<li><p><cite>CONCURRENT_REQUESTS_PER_DOMAIN</cite> Defaults to 8, and controls the number of
simultaneous requests to be performed for each domain. You might want to
lower this if you don‚Äôt want to put too much pressure on the website‚Äôs
server, and you probably don‚Äôt want to get blocked!</p></li>
<li><p><cite>DEFAULT_REQUEST_HEADERS</cite> You can change this if you need to.</p></li>
<li><p><cite>DEPTH_LIMIT</cite> How deep your crawl will be allowed. The default has no limit.</p></li>
<li><p><cite>DOWNLOAD_DELAY</cite> Similar to the first option. Controls the amount of time in
seconds for the crawler to wait between consecutive pages of the same website.
It can also take fractions of a second (0.4, 0.75, etc.)</p></li>
<li><p><cite>LOG_FILE</cite> If you want to save your crawl logs to a file, you can provide a
path to it here.</p></li>
<li><p><cite>USER_AGENT</cite> If you want to identify yourself differently while crawling.
This is affected by the robots.txt rules, so you would be potentially
allowed/disallowed from certain pages based on your user-agent.</p></li>
<li><p><cite>CLOSESPIDER_ERRORCOUNT</cite>, <cite>CLOSESPIDER_ITEMCOUNT</cite>, <cite>CLOSESPIDER_PAGECOUNT</cite>,
<cite>CLOSESPIDER_TIMEOUT</cite> Stop crawling after that many errors, items, pages, or
seconds. These can be very useful to limit your crawling in certain cases.
I particularly like to use <cite>CLOSESPIDER_PAGECOUNT</cite> when exploring a new
website, and also to make sure that my selectors are working as expected. So
for your first few crawls you might set this to one hundred for example and
explore the crawled pages. Then when you are confident things are working
fine, you can remove this restriction. <cite>CLOSESPIDER_ERRORCOUNT</cite> can also be
very useful while exploring, just in case you get unexpected errors.</p></li>
</ul>
<p><strong>Usage</strong></p>
<p>A very simple dictionary to be added to your function call:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;http://exmaple.com&#39;</span><span class="p">,</span> <span class="s1">&#39;outpuf_file.jl&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="n">custom_settings</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;CLOSESPIDER_PAGECOUNT&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="s1">&#39;CONCURRENT_REQUESTS_PER_DOMAIN&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="s1">&#39;USER_AGENT&#39;</span><span class="p">:</span> <span class="s1">&#39;custom-user-agent&#39;</span><span class="p">})</span>
</pre></div>
</div>
<p>Please refer to the <a class="reference external" href="https://docs.scrapy.org/en/latest/topics/settings.html">spider settings documentation</a>
for the full details.</p>
<dl class="py function">
<dt id="advertools.spider.crawl">
<code class="sig-name descname">crawl</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">url_list</span></em>, <em class="sig-param"><span class="n">output_file</span></em>, <em class="sig-param"><span class="n">follow_links</span><span class="o">=</span><span class="default_value">False</span></em>, <em class="sig-param"><span class="n">css_selectors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">xpath_selectors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">custom_settings</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">allowed_domains</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/spider.html#crawl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#advertools.spider.crawl" title="Permalink to this definition">¬∂</a></dt>
<dd><p>Crawl a website‚Äôs URLs based on the given <code class="xref py py-attr docutils literal notranslate"><span class="pre">url_list</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>url_list</strong> (<em>url</em><em>,</em><em>list</em>) ‚Äì One or more URLs to crawl. If <code class="docutils literal notranslate"><span class="pre">follow_links</span></code>
is True, the crawler will start with these URLs and
follow all links on pages recursively.</p></li>
<li><p><strong>output_file</strong> (<em>str</em>) ‚Äì The path to the output of the crawl. Jsonlines only
is supported to allow for dynamic values. Make sure
your file ends with ‚Äú.jl‚Äù, e.g. <cite>output_file.jl</cite>.</p></li>
<li><p><strong>follow_links</strong> (<em>bool</em>) ‚Äì Defaults to False. Whether or not to follow links
on crawled pages.</p></li>
<li><p><strong>css_selectors</strong> (<em>dict</em>) ‚Äì A dictionary mapping names to CSS selectors. The
names will become column headers, and the
selectors will be used to extract the required
data/content.</p></li>
<li><p><strong>xpath_selectors</strong> (<em>dict</em>) ‚Äì A dictionary mapping names to XPath selectors.
The names will become column headers, and the
selectors will be used to extract the required
data/content.</p></li>
<li><p><strong>custom_settings</strong> (<em>dict</em>) ‚Äì A dictionary of optional custom settings that
you might want to add to the spider‚Äôs
functionality. There are over 170 settings for
all kinds of options. For details please
refer to the <a class="reference external" href="https://docs.scrapy.org/en/latest/topics/settings.html">spider settings</a>
documentation.</p></li>
<li><p><strong>allowed_domains</strong> (<em>list</em>) ‚Äì (optional) A list of the allowed domains to
crawl. This ensures that the crawler does not
attempt to crawl the whole web. If not
specified, it defaults to the domains of the
URLs provided in <code class="docutils literal notranslate"><span class="pre">url_list</span></code>. You can use it
for sub-domains if you want them to be crawled
as they will not be crawled if not specified.</p></li>
</ul>
</dd>
<dt class="field-even">Examples</dt>
<dd class="field-even"><p></p></dd>
</dl>
<p>Crawl a website and let the crawler discover as many pages as available</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span> <span class="n">follow_links</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">crawl_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Crawl a known set of pages (on a single or multiple sites) without
following links (just crawl the specified pages) or ‚Äúlist mode‚Äù:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">([</span><span class="s1">&#39;http://exmaple.com/product&#39;</span><span class="p">,</span> <span class="s1">&#39;http://exmaple.com/product2&#39;</span><span class="p">,</span>
<span class="gp">... </span>       <span class="s1">&#39;https://anotherexample.com&#39;</span><span class="p">,</span> <span class="s1">&#39;https://anotherexmaple.com/hello&#39;</span><span class="p">],</span>
<span class="gp">... </span>       <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span> <span class="n">follow_links</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Crawl a website, and in addition to standard SEO elements, also get the
required CSS selectors.
Here we will get three additional columns <cite>price</cite>, <cite>author</cite>, and
<cite>author_url</cite>. Note that you need to specify if you want the text attribute
or the <cite>href</cite> attribute if you are working with links (and all other
selectors).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="s1">&#39;output_file.jl&#39;</span><span class="p">,</span>
<span class="gp">... </span>      <span class="n">css_selectors</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="s1">&#39;.a-color-price::text&#39;</span><span class="p">,</span>
<span class="gp">... </span>                     <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;.contributorNameID::text&#39;</span><span class="p">,</span>
<span class="gp">... </span>                     <span class="s1">&#39;author_url&#39;</span><span class="p">:</span> <span class="s1">&#39;.contributorNameID::attr(href)&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="advertools.serp.html" class="btn btn-neutral float-right" title="Import Search Engine Results Pages (SERPs) for Google and YouTube" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="advertools.sitemaps.html" class="btn btn-neutral float-left" title="Download, Parse, and Analyze XML Sitemaps" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Elias Dabbas

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>